{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "lab8-zbiory-danych-ner.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0C-0nUIP6Rm"
      },
      "source": [
        "# Tworzenie zasobów + wykrywanie encji nazwanych\n",
        "\n",
        "Algorytmy wykorzystywane w problemach przetwarzania języka naturalnego opierają najczęściej swoje działanie o analizę dużych korpusów danych. O ile w zadaniach konkursowych często odpowiednie dane są już przygotowane, o tyle tworząc własne eksperymenty, często musimy sami pozyskać dane i przetransformować do użytecznej postaci.\n",
        "\n",
        "Dzisiejsze laboratoria dotyczyć będą tworzenia korpusów danych, tworzenia reprezentacji CoNNL i wykorzystania jej do zadania wykrywania encji nazwanych.\n",
        "\n",
        "## Automatyczne pozyskiwanie surowych danych tekstowych\n",
        "Dotychczas omawiane metody działały na surowym tekście, który transformowany był do odpowiedniej reprezentacji wektorowej (Bag of words, bag of ngrams, embeddingi). Jak zautomatyzować pozyskiwanie takich surowych danych z internetu?\n",
        "\n",
        "W tej części skupimy się na stworzeniu automatycznego pobieracza danych, który działać będzie w dwóch \"obszarach\":\n",
        "<ol>\n",
        "<li>crawler: moduł odwiedzający kolejne strony internetowy</li>\n",
        "<li>scraper: moduł ekstrahujący treść z konkretnych stron internetowych</li>\n",
        "</ol>\n",
        "\n",
        "Wykorzystajmy do tego dwie biblioteki: \n",
        "\n",
        "**urllib** - do odwiedzania stron\n",
        "\n",
        "**BeautifulSoup** - do parsowania danych (np. w formacie HTML).\n",
        "\n",
        "## Zadanie1 (2pkt): Napisz prosty ekstraktor danych ze stron WWW odwiedzający kilka podstron\n",
        "Ekstraktor ma odwiedzić zadaną stronę internetową, pobrać zawartość wszystkich tekstów wewnątrz paragrafów (wewnątrz tagów P zawartych w pobranym dokumencie HTML), a następnie odwiedzić 5 dowolnych linków z tej strony i z nich analogicznie pobrać zawartość.\n",
        "Łącznie powinniśmy otrzymać dane z 6 adresów internetowch (strona główna + 5 linków ze strony głównej).\n",
        "\n",
        "Do napisania crawlera przydać się mogą następujące funkcje:\n",
        "\n",
        "urllib.request.urlopen() - do pobrania zawartości strony\n",
        "findAll() na obiekcie BeautifulSoup, można ją wykorzystać do przeiterowania po wszystkich tagach danego rodzaju\n",
        "get_text() - Istnieje duża szansa, że wewnątrz tagów P znajdą się również inne tagi HTML, chcielibyśmy oczyścić \n",
        "z nich tekst. Można to zrobić albo z wyrażeniami regularnymi (robiliśmy takie zadanie na pierwszych laboratoriach!), albo użyć właśnie funkcji get_text() z BeautifulSoup\n",
        "\n",
        "Linki do dokumentacji:\n",
        "urllib, pobieranie danych: https://docs.python.org/3/howto/urllib2.html\n",
        "beautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ (przeczytanie QuickStart jest wystarczające do zrobienia tego zadania)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb01x7KsP6Rt",
        "outputId": "b50646f4-2eb9-47c8-c244-11cd6c0e7141"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def generate_links(root, page):\n",
        "  res = [x['href'] for x in page.find_all('a', href=True)]\n",
        "  return [x if re.match(r'^https?:\\/\\/', x) else urljoin(root, x) for x in res if x != root]\n",
        "\n",
        "def get_data(page):\n",
        "  res = [x.get_text().strip() for x in page.find_all('p')]\n",
        "  return [x for x in res if x]\n",
        "  \n",
        "root = 'https://eurosport.tvn24.pl/'\n",
        "page = BeautifulSoup(urlopen(root).read())\n",
        "data = get_data(page)\n",
        "\n",
        "for url in generate_links(root, page)[:5]:\n",
        "  print('scraping', url)\n",
        "  subpage = BeautifulSoup(urlopen(url).read())\n",
        "  data += get_data(subpage)\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scraping https://tvn24.pl/\n",
            "scraping https://tvn24.pl/swiat/\n",
            "scraping https://tvn24.pl/tvnwarszawa/\n",
            "scraping https://tvn24.pl/tvnmeteo/\n",
            "scraping https://fakty.tvn24.pl/\n",
            "['2:0 (2:1)', '0:0', '\\n', 'W poniedziałek Dania poinformowała o wykluczeniu szczepionki Johnson & Johnson z programu szczepień.', 'Młodszy aspirant Michał Kędzierski zginął we wtorek rano podczas interwencji przy ulicy Chełmońskiego w Raciborzu.', 'Lekarze byli zaniepokojeni stanem zdrowia kobiety i jej dzieci.', 'Z 233 posłów PiS głos \"przeciw\" oddało 229, a trzech nie głosowało.', 'Maturzyści muszą w tym roku przystąpić do trzech egzaminów pisemnych na poziomie podstawowym. To był jeden z nich.', '76-latek był w ciężkim stanie.', 'Najnowsze dane Ministerstwa Zdrowia.', 'Dane w artykule są codziennie aktualizowane.', 'Wojciech Andrusiewicz w TVN24.', 'Dziecko zmarło w trakcie operacji w szpitalu.', 'Posłowie Solidarnej Polski zagłosowali we wtorek przeciwko ratyfikacji zasobów własnych.', 'Szef Komitetu Wykonawczego PiS Krzysztof Sobolewski w wywiadzie.', 'Komunikat spółki.', 'Synoptycy Instytutu Meteorologii i Gospodarki Wodnej wydali alarmy pierwszego stopnia.', 'Do wypadku doszło w Biszczy w powiecie biłgorajskim (woj. lubelskie).', 'Pisze środowy \"Dziennik Gazeta Prawna\".', 'Oświadczył brytyjski regulator leków i produktów ochrony zdrowia.', 'Władze Wielkiej Brytanii chcą całkowicie wyeliminować ryzyko zakażenia koronawirusem do świąt Bożego Narodzenia.', 'Konferencja Borysa Budki i Tomasza Grodzkiego.', 'To podstawa do wypłaty pieniędzy z unijnego Funduszu Odbudowy. Z tej puli Polska ma mieć do dyspozycji około 58 miliardów euro.', '\"Bardzo się cieszę, że możemy zainicjować nasz bardzo ciekawy pomysł\".', 'Była poszukiwana listami gończymi.', 'W sprawie zarzuty usłyszały trzy osoby.', 'Reportaż Dominiki Ziółkowskiej.', 'W położonym na południu Brazylii stanie Santa Catarina.', 'Przez sąd w Oslo.', 'Fizjoterapeuci obawiają się, że prywatne zabiegi staną się codziennością, bo NFZ zmienia ich wycenę.', 'Materiał Patryka Rabiegi.', 'Piłkarze PSG oskarżają sędziego. Nie chodzi o błędne decyzje, a słowa.', 'Zmierzyła się z nim pani Aleksandra Kołodziejczyk z Mińska Mazowieckiego.', 'Osy to owady, które nie cieszą się zbyt dobrą opinią. Okazuje się jednak, że ich rola w ekosystemie jest niedoceniana.', 'Cykady Brood X to wyjątkowe owady.', 'Półtoraroczna Aiden James rozbawiła swoich rodziców do łez.', 'Dokument z biblioteki discovery+.', '\\n', 'Władze Wielkiej Brytanii chcą całkowicie wyeliminować ryzyko zakażenia koronawirusem do świąt Bożego Narodzenia.', 'Przekazał wiceprzewodniczący Rady Zarządzającej Europejskiej Agencji Leków doktor Grzegorz Cessak.', '25-latka urodziła dziewięcioraczki.', 'Przyszłość polityczna wieloletniego szefa izraelskiego rządu - jak prognozują analitycy - jest niepewna.', 'W środę zarząd Facebooka ma podjąć decyzję, czy podtrzymać bezterminowe zawieszenie Donalda Trumpa na platformie.', 'Nowy projekt władz chińskiej prowincji.', \"20 kwietnia ława przysięgłych uznała byłego policjanta winnym zabójstwa George'a Floyda.\", 'Wpis ministra spraw zagranicznych na Twitterze.', 'Od kilku dni przez południowo-wschodnią część USA przetaczają się gwałtowne burze i tornada. Zginęły trzy osoby, a co najmniej sto tysięcy mieszkańców nie miało prądu. Wiatr był tak silny, że niszczył domy i przewracał ciężarówki na autostradach.', 'Ukraiński minister ochrony zdrowia ocenił, że w ten sposób Moskwa chce przeforsować własny preparat Sputnik V.', 'W położonym na południu Brazylii stanie Santa Catarina.', 'Oficjalne wyniki po przeliczeniu 99,7 procent głosów.', 'Przez sąd w Oslo.', '23 osoby zginęły, a ponad 70 zostało rannych.', 'Cykady Brood X to wyjątkowe owady. Pierwsze 17 lat spędzają w glebie, gdzie dorastają, by potem wyjść na powierzchnię, rozmnożyć się i po prostu zginąć. We wschodnich regionach Stanów Zjednoczonych właśnie zbliża się czas, kiedy wychodzą z ukrycia.', 'Ma opowiadać o relacji między ojcem a synem.', 'Oświadczył brytyjski regulator leków i produktów ochrony zdrowia.', 'W związku z masowymi szczepieniami dorosłych.', 'Mario Draghi podczas narady ministrów turystyki krajów G20.', '\\n', 'Ptak chodzi po podwórkach.', 'Przypłyną statkiem.', 'Skrzyżowanie Żytniej i Karolkowej na Woli.', 'Była poszukiwana listami gończymi.', 'Jak podają śledczy, próbowali wsiąść do samolotu na Lotnisku Chopina.', 'W garażach przy Krasińskiego.', 'Zablokowane pasy w kierunku Mokotowa.', '\"To walka Dawida z Goliatem\".', 'Wrażeniami podzielili się maturzyści z XXX Liceum Ogólnokształcącego im. Jana Śniadeckiego na Woli.', 'Trzy ogniska i 16 osób zakażonych nową mutacją w całym kraju.', 'Funkcjonariusz z Mińska Mazowieckiego zawieszony, sprawę bada prokuratura.', 'Policja w Siedlcach szuka podejrzewanego.', 'Które placówki będą otwarte i od kiedy?', 'Potrzebna jest dzierżawa gruntu.', 'Dane komendy stołecznej.', 'Zabezpieczono niemal 130 urządzeń.', '\\n', 'Środa, 05.05.2021', 'Dziś, Środa', '5 maja', '14°C', 'Temp. aktualna:', 'Dziś, Środa', 'najbliższe 24 godziny', 'Przelotne opady', 'Pogoda na dziś, na jutro\\nPogoda długoterminowa', 'Synoptycy Instytutu Meteorologii i Gospodarki Wodnej wydali alarmy pierwszego stopnia. Zagrożeniem mają być burze i silne porywy wiatru. Miejscami ich prędkość może wynosić do 90 kilometrów na godzinę.czytaj dalej', 'Osy to owady, które nie cieszą się zbyt dobrą opinią. Okazuje się jednak, że ich rola w ekosystemie jest niedoceniana. Jak wynika z analizy brytyjskich naukowców, są równie pożyteczne, co \"bardziej przyjazne\" pszczoły.czytaj dalej', 'Od kilku dni przez południowo-wschodnią część USA przetaczają się gwałtowne burze i tornada. Zginęły trzy osoby, a co najmniej sto tysięcy mieszkańców nie miało prądu. Wiatr był tak silny, że niszczył domy i przewracał...czytaj dalej', 'Na kołobrzeskiej plaży pojawiła się młoda i wycieńczona foka. Dzięki szybkiej reakcji i interwencji Błękitnego Patrolu, zwierzę zostało przetransportowane do helskiego Fokarium Stacji Morskiej Uniwersytetu Gdańskiego.czytaj dalej', 'W środę w wielu regionach należy spodziewać się burz. Wyładowaniom towarzyszyć mają dość silne opady i porywy wiatru. Termometry pokażą maksymalnie od 12 do 21 stopni Celsjusza.czytaj dalej', 'Cykady Brood X to wyjątkowe owady. Pierwsze 17 lat spędzają w glebie, gdzie dorastają, by potem wyjść na powierzchnię, rozmnożyć się i po prostu zginąć. We wschodnich regionach Stanów Zjednoczonych właśnie zbliża się czas, kiedy...czytaj dalej', 'W środę odczuwać będziemy komfort termiczny albo ciepło.czytaj dalej', 'Strażacy ze stanu Kalifornia przetarli oczy ze zdumienia, gdy odkryli, że w kominie w mieście Montecito utknęło około tysiąca niewielkich ptaków. Co ciekawe, podobna sytuacja zdarzyła się kilka dni wcześniej, ale w innym mieście.czytaj dalej', 'Ulewy z silnym wiatrem nawiedziły gminę Chongqing w środkowej części Chin. Doszło do podtopień, wiatr powalił na drogi łącznie ponad 100 drzew. Opadom towarzyszył grad.czytaj dalej', 'W najbliższą środę przypada Światowy Dzień Astmy. Na tę chorobę cierpi mnóstwo Polaków, jednak tylko 30 procent z nich potrafi ją kontrolować. Eksperci podpowiadają, co robić, by życie z astmą było jak najprostsze.czytaj dalej', 'Walijska policja prowadzi dochodzenie w sprawie zniszczenia chronionego gniazda rybołowów nad jeziorem. W gnieździe były świeżo złożone jaja. Na nagraniu słychać piłę łańcuchową, a następnie widać, jak platforma lęgowa upada do...czytaj dalej', 'Trzeci raz w ciągu czterech lat w jednym z mieszkań w Georgii w Stanach Zjednoczonych zadomowiły się pszczoły. Pod koniec kwietnia w suficie znaleziono kolonię ponad 100 tysięcy owadów.czytaj dalej', 'Mieszkańcy południowej części Stanów Zjednoczonych doświadczają gwałtownej pogody. W poniedziałek zniszczenia spowodowało kilka tornad i silne burze. Jedna osoba została przygnieciona drzewem, spadł grad o średnicy około czterech...czytaj dalej', 'Przez kilkanaście dni na plaży w miejscowości Morbylanga na szwedzkiej wyspie Olandia leżało ciało ośmiometrowego długopłetwca oceanicznego (Megaptera novaeangliae). W poniedziałek podjęto decyzję o usunięciu zwierzęcia, z obawy...czytaj dalej', 'W amerykańskim stanie Michigan złowiono ważącego ponad 100 kilogramów i mierzącego ponad dwa metry jesiotra. \"To jeden z największych jesiotrów jeziornych odnotowanych w Stanach Zjednoczonych\" - poinformowała w mediach...czytaj dalej', 'Ponad 100 odcisków dłoni dzieci odkryto w jaskini na meksykańskim półwyspie Jukatan. Znalezisko rzuca nowe światło na rytuały Majów - podkreśla Reuters. Według jednego z archeologów kolory śladów są wskazówką do ich znaczenia.czytaj dalej', 'Wiatr przewrócił drzewo na taras widokowy w parku w Białej Podlaskiej. - Działania były dosyć trudne ze względu na wielkość drzewa – relacjonował młodszy aspirant Dariusz Derlukiewicz z Jednostki Ratowniczo-Gaśniczej Państwowej...czytaj dalej', 'Tornada przeszły w niedzielę przez trzy miasta w amerykańskim stanie Missisipi. Żywioł zrywał dachy z domów, łamał drzewa i niszczył linie energetyczne.czytaj dalej', 'Oto najbardziej społecznościowa pogoda w Polsce. W nowej aplikacji TVN Meteo znajdziesz nie tylko aktualne prognozy, informacje i wideo, ale będziesz także mógł podzielić się pogodą z innymi w portalach społecznościowych.czytaj dalej', 'Zostań Reporterem 24 - wyślij nam swój materiał przez Kontakt24 lub\\r\\n                    kontakt24@tvn.pl!', 'Po chłodnym długim weekendzie temperatura zacznie stopniowo rosnąć, a za tydzień...', 'Polska jest pod wpływem wyżu Nicole znad Rumunii. Napływa do nas ciepłe powietrz...', 'Nastała astronomiczna wiosna. Jednak patrząc na piątkową pogodę, zimowa aura nie...', 'Copyrights © 2021 TVN S.A', 'Copyright (C) 1997-2021 Korzystanie z materiałów redakcyjnych TVN S.A. / TVN Media Sp. z o.o. wymaga wcześniejszej zgody TVN S.A./ TVN Media Sp. z o.o. oraz zawarcia stosownej umowy licencyjnej. Na podstawie art. 25 ust. 1 pkt. 1 b) ustawy o prawie autorskim i prawach pokrewnych TVN S.A. / TVN Media Sp. z o.o. wyraźnie zastrzega, że dalsze rozpowszechnianie artykułów zamieszczonych w programach oraz na stronach internetowych TVN S.A. / TVN Media Sp. z o.o. jest zabronione', '\\n', 'Śledź nas na:', 'Jak oglądać \"Fakty\" w internecie?', 'zobacz wszystkich &raquo', 'Głosowanie nad udzieleniem zgody na ratyfikację unijnego funduszu odbudowy to nie tylko kwestia sporna w Zjednoczonej...', 'Od blisko dwóch tygodni w Indiach każdego dnia notuje się ponad 300 tysięcy nowych zakażeń SARS-CoV-2, a zaledwie...', 'Rezultat wyborów w Rzeszowie może być ważną wskazówką dla całego kraju. Czworo kandydatów na urząd prezydenta stolicy...', 'Niedziela przyniosła dynamiczną, burzową pogodę. Miejscami, tak jak w gminie Tereszpol na Lubelszczyźnie, pojawiły...', '4 maja mają w Polsce rozpocząć się egzaminy maturalne. Na początku maturzyści zmierzą się z językiem polskim....', 'Przełom zimy i wiosny przyniósł dramatyczną trzecią falę epidemii, ale że wszystko wskazuje na jej koniec, to od 4...', 'Oczekiwanie w deszczu i kilkusetmetrowe kolejki stały się symbolem majówkowej akcji szczepień. Taką formę akcji...', 'Mija 230 lat od uchwalenia przez Sejm Wielki Konstytucji 3 maja. W centralnych uroczystościach w Warszawie - poza...', 'W Zjednoczonej Prawicy panuje dwugłos odnośnie unijnych funduszy. PiS i Porozumienie są \"za\", Solidarna Polska...', 'Widział zarówno włoską, jak i polską odsłonę pandemii. Dlatego udziela wsparcia pacjentom chorym na COVID-19 i ich...', 'ul. Wiertnicza 16602-952 WarszawaTel. (22) 856 6161', 'Copyright (C) 1997-2021 Korzystanie z materiałów redakcyjnych TVN S.A. / TVN Media Sp. z o.o. wymaga wcześniejszej zgody TVN S.A./ TVN Media Sp. z o.o. oraz zawarcia stosownej umowy licencyjnej. Na podstawie art. 25 ust. 1 pkt. 1 b) ustawy o prawie autorskim i prawach pokrewnych TVN S.A. / TVN Media Sp. z o.o. wyraźnie zastrzega, że dalsze rozpowszechnianie artykułów zamieszczonych w programach oraz na stronach internetowych TVN S.A. / TVN Media Sp. z o.o. jest zabronione', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgHLx-6P6Ru"
      },
      "source": [
        "# Zadanie 2 - CONLL\n",
        "Dane ustrukturyzowane w formacie CONLL.\n",
        "\n",
        "Niektóre algorytmy korzystają z dodatkowych metadanych opisujących poszczególne tokeny (słowa). Bardzo popularnym formatem zapisu takich danych jest format CONLL. \n",
        "\n",
        "Reprezentacja CONLL polega na tym, że dany tekst dzielony jest na zdania, a następnie każde zdanie dzielone jest na tokeny (tokenizowane). Następnie dla każdego tokenu tworzymy listę opisującą cechy tego tokenu (słowa).\n",
        "Poniżej przykład wektora opisującego każdy token zadanego tekstu:\n",
        "<ol>\n",
        "    <li>ID - numer porządkowy tokenu w zdaniu</li>\n",
        "    <li>text - tekst tokenu w formie nieprzetworzonej</li>\n",
        "    <li>Part of Speech tag (POS tag) - informacja o części mowy, która powiązana jest z tym słowem </li>\n",
        "    <li>is digit - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest liczbą</li>\n",
        "    <li>is punct - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest znakiem interpunkcyjnym</li>\n",
        "</ol>\n",
        "\n",
        "Wektory cech dla kolejnych słów zapisywane są pod sobą. **Separatorem cech w wektorze jest pojedyncza spacja.**\n",
        "\n",
        "**Zdania zwyczajowo oddzielamy od siebie podwójnym znakiem nowej linii.**\n",
        "\n",
        "Historycznie CONLL był bardzo konkretnym formatem danych w którym mieliśmy z góry narzucone cechy (np. format CONLL-U https://universaldependencies.org/docs/format.html). Liczba cech ewoluowała jednak w czasie i w wielu miejscach CONLL stał się synonimem ogólnego formatu, w którym dobór cech zależy tylko od nas, jednak stałym jest zapis sekwencji tokenów jako sekwencji wierszy w tekście, gdzie każdy wiersz jest listą oddzielonych spacją wartości (cech), a zdania oddzielone są od siebie podwójnym znakiem nowej linii.\n",
        "\n",
        "\n",
        "### Przykład:\n",
        "\n",
        "Tekst: Kasia kupiła 2 lizaki: truskawkowy i zielony. Kasia używa Apple IPhone 5 i IPad.\n",
        "\n",
        "Reprezentacja CONLL **(spacje separujące kolumny zostały zwielokrotnione na potrzeby zwiększenia czytelności)**\n",
        "<pre>\n",
        "1 Kasia  RZECZOWNIK 0 0\n",
        "2 kupiła CZASOWNIK  0 0\n",
        "3 2      LICZEBNIK  1 0\n",
        "4 lizaki RZECZOWNIK 0 0\n",
        "5 .      _          0 1\n",
        "\n",
        "1 Kasia  RZECZOWNIK 0 0\n",
        "2 używa  CZASOWNIK  0 0\n",
        "3 Apple  RZECZOWNIK 0 0\n",
        "4 IPhone RZECZOWNIK 0 0\n",
        "5 5      LICZEBNIK  1 0\n",
        "6 i      SPÓJNIK    0 0\n",
        "7 iPad   RZECZOWNIK 0 0\n",
        "8 .      _          0 1\n",
        "</pre>\n",
        "\n",
        "**Zadanie 2a (0.5 pkt)**: Napisz funkcję, która z zadanego tekstu w formie surowego tekstu stworzy reprezentację CONLL opisaną wcześniej wymienionymi atrybutami (ID, text, POS-tag, is_digit, is_punct).\n",
        "\n",
        "Wykorzystaj sentence splitter i tokenizator z NLTK. Do uzyskania informacji o POS-tagach każdego tokenu wykorzystaj funkcję nltk.pos_tag(). W kolumnie związanej z POS-tagiem zapisz pos tag w takiej formie, w jakiej uzyskamy go z funkcji pos_tag (pos_tag() zwraca formy skrótowe, np. 'NN' dla rzeczowników), nie trzeba więc zamieniać napisu \"NN\" na \"RZECZOWNIK\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7_FNcQfP6Ru",
        "outputId": "b2e1ebb9-6621-402c-a31d-6fc6e5288177"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def generate_conll(text):\n",
        "    i = 1\n",
        "    for sentence in sent_tokenize(text):\n",
        "      for word in word_tokenize(sentence):\n",
        "        tag = pos_tag(word)[0][1]\n",
        "        digit = int(tag == 'CD')\n",
        "        punct = int(tag in ',.?!')\n",
        "        if punct:\n",
        "          tag = '_'\n",
        "        print(i, word, tag, digit, punct)\n",
        "        i += 1\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "generate_conll(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Kate NNP 0 0\n",
            "2 uses JJ 0 0\n",
            "3 IPhone PRP 0 0\n",
            "4 5 CD 1 0\n",
            "5 and DT 0 0\n",
            "6 IPad PRP 0 0\n",
            "7 . _ 0 1\n",
            "\n",
            "\n",
            "8 Kate NNP 0 0\n",
            "9 bought NN 0 0\n",
            "10 2 CD 1 0\n",
            "11 lolipops NN 0 0\n",
            "12 . _ 0 1\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "TLcAyF-nP6Rv"
      },
      "source": [
        "\n",
        "Wyobraźmy sobie teraz, że chcielibyśmy wykrywać wzmianki o urządzeniach elektronicznych w tekście. W jaki sposób zakodować informację o (potencjalnie wielotokenowych) nazwach produktów w CONLL, tak, aby później móc wykonać proces uczenia?\n",
        "\n",
        "Dodajmy w naszym CONLLu dodatkową kolumnę reprezentującą informację o urządzeniach elektronicznych.\n",
        "Nazwy urządzeń mogą składać się potencjalnie z wielu słów.\n",
        "Do zakodowania wielotokenowych tekstów używa się najczęściej notacji IOB, gdzie każda literka skrótu oznacza interpretację aktualnego słowa:\n",
        "<ul>\n",
        "    <li> B = begin, marker, który mówi, że aktualne słowo to początek nazwy </li>\n",
        "    <li> I = inside, marker, który mówi, że aktualne słowo to kontynacja nazwy, która rozpoczyna się wystąpieniem wcześniejszego B</li>\n",
        "    <li> O = outside, marker, który mówi, że aktualne słowo nie jest interesującą nas nazwą (jest poza nią) </li>\n",
        "</ul>\n",
        "\n",
        "Po dodaniu nowej kolumny (na końcu) nasz CONLL przybiera postać:\n",
        "\n",
        "<pre>\n",
        "1 Kasia  RZECZOWNIK 0 0 O\n",
        "2 kupiła CZASOWNIK  0 0 O\n",
        "3 2                 1 0 O\n",
        "4 lizaki RZECZOWNIK 0 0 O\n",
        "5 .      _          0 1 O\n",
        "\n",
        "1 Kasia  RZECZOWNIK 0 0 O\n",
        "2 używa             0 0 O\n",
        "3 Apple  RZECZOWNIK 0 0 B\n",
        "4 IPhone RZECZOWNIK 0 0 I\n",
        "5 5                 1 0 I\n",
        "6 i      SPÓJNIK    0 0 O\n",
        "7 iPad   RZECZOWNIK 0 0 B\n",
        "8 .      _          0 1 0\n",
        "</pre>\n",
        "\n",
        "Zwróćcie Państwo uwagę na ostatnią kolumnę, czytając tekst od góry w dół, wystąpienie literki \"B\" oznacza początek interesującej frazy (Apple), jeśli zaraz za \"B\" pojawia się sekwencja oznaczona jako \"I\" - kolejne tokeny stanowią kontynuację interesującej nas frazy, w tym przypadku 3 tokeny \"Apple IPhone 5\" tworzą jeden byt. Poza tym widzimy, że \"iPad\" stanowi osobny, jednotokenowy byt.\n",
        "\n",
        "Po co rozróżniać pomiędzy \"B\", \"I\" i \"O\", czy nie można uwzględnić tylko dwóch tagów \"wewnątrz frazy\", \"poza frazą\"? Teoretycznie można, ale wprowadzimy w ten sposób sytuacje niejednoznaczne. \n",
        "\n",
        "Sprawdźmy to na przykładzie sekwencji \"XBox Playstation\" reprezentującej 2 osobne byty. Używając tagowania IOB nasza sekwencja wyglądałaby tak:\n",
        "\n",
        "XBox B\n",
        "PlayStation B\n",
        "\n",
        "Widzimy więc, że dwa tagi \"B\" oznaczają dwa początki osobnych fraz. Co jednak gdybyśmy używali tagów \"wewnątrz (interesującej nas) frazy\", \"poza (interesującą nas) frazą\"?\n",
        "\n",
        "XBox \"wewnątrz (interesującej nas) frazy\"\n",
        "Playstation \"wewnątrz (interesującej nas) frazy\"\n",
        "\n",
        "W tej sytuacji oznaczyliśmy poprawnie oba tokeny jako części interesujących nas fraz. Jednak nie wiemy, czy XBox Playstation to jedna, czy dwie osobne frazy (byty) -- stąd format IOB jest zdecydowanie bezpieczniejszym wyborem.\n",
        "\n",
        "**Zadanie 2b (0.5 pkt)**: Napisz funkcję, która wygeneruje CONLL z uwzględnieniem tagów IOB dotyczących urządzeń.\n",
        "Nasza funkcja posiada teraz dodatkowy argument devices, który zawiera listę obiektów, które opisują gdzie (przesunięcie znakowe) znajduje się początek i koniec wzmianek.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9G3MAMeP6Rw",
        "outputId": "9add20a5-11f3-4cee-ce60-45f4ba7dee23"
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "def generate_CONLL(text, devices=[]):\n",
        "  device = 0\n",
        "  i = 1\n",
        "  span_generator = WhitespaceTokenizer().span_tokenize(text)\n",
        "  spans = [span for span in span_generator]\n",
        "  offsets = [span[0] for span in spans]\n",
        "  for sentence in sent_tokenize(text):\n",
        "    for word in word_tokenize(sentence):\n",
        "      device_tag = 'O'\n",
        "      if device < len(devices):\n",
        "        begin, end = devices[device]['begin'], devices[device]['end']\n",
        "        if begin == offsets[i-1]:\n",
        "          device_tag = 'B'\n",
        "        elif begin < offsets[i-1] < end:\n",
        "          device_tag = 'I'\n",
        "        if end <= offsets[i-1] + len(word):\n",
        "          device += 1\n",
        "      tag = pos_tag(word)[0][1]\n",
        "      digit = int(tag == 'CD')\n",
        "      punct = int(tag in ',.?!')\n",
        "      if punct:\n",
        "        tag = '_'\n",
        "      print(i, word, tag, digit, punct, device_tag)\n",
        "      i += 1\n",
        "    print('\\n')\n",
        "\n",
        "# parametr devices to lista słowników w którym mamy informację o numerze znaku na którym fraza się zaczyna i kończy (zobacz: próba wywołania w ostatniej linijce) (litera I z Iphone występuje na 10 znaku)\n",
        "# Zapoznaj się z dokumentacją SpaCy (obiekt Token), aby zobaczyć jak wydobyć informację o pozycji danego słowa w zdaniu/dokumencie.\n",
        "    \n",
        "generate_CONLL(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\", devices=[{\"begin\": 10, \"end\":18}, {\"begin\": 23, \"end\": 27}])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Kate NNP 0 0 O\n",
            "2 uses JJ 0 0 O\n",
            "3 IPhone PRP 0 0 B\n",
            "4 5 CD 1 0 I\n",
            "5 and DT 0 0 O\n",
            "6 IPad PRP 0 0 B\n",
            "7 . _ 0 1 O\n",
            "\n",
            "\n",
            "8 Kate NNP 0 0 O\n",
            "9 bought NN 0 0 O\n",
            "10 2 CD 1 0 O\n",
            "11 lolipops NN 0 0 O\n",
            "12 . _ 0 1 O\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Q1OM3YP6Rw"
      },
      "source": [
        "Często chcemy w tekście naraz oznaczać byty, które należą do różnych kategorii, np. lokacje, numery telefonów, daty, wzmianki o osobach. W takich sytuacjach używa się również kodowania IOB jednak wzbogaca się etykiety o odpowiednie informacje używając formatu:\n",
        "\n",
        "{tag IOB}-{etykieta kategorii}\n",
        "\n",
        "Stąd daty przyjmują oznaczenia: B-DATE / I-DATE, osoby B-PERSON / I-PERSON, numery telefonów B-PHONENUMBER / I-PHONENUMBER, lokacje: B-LOCATION / I-LOCATION itp. Wiemy zatem czy dany token należy do interesującej nas frazy i do jakiej kategorii przypisana jest ta fraza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZoKzqklP6Rx"
      },
      "source": [
        "## Wykrywanie encji nazwanych (Named Entity Recognition - NER)\n",
        "\n",
        "Dotychczas na większości zajęć rozważaliśmy problem klasyfikacji, w którym całym dokumentom przypisywalśmy pojedynczą etykietę (sentyment związany z dokumentem, informacja o tym, czy tekst jest spamowy, etykieta mówiąca o tym w jakim języku napisany jest dokument). Warto jednak również wspomnieć o tzw. tagowaniu sekwencji, które dla każdego elementu sekwencji (słowa) nadaje odpowiednią etykietę.\n",
        "\n",
        "Gdzie taka procedura ma zastosowanie? Wymieńmy kilka przykładów \n",
        "<ol>\n",
        "    <li>Wykrywanie wyrażeń dotyczących miejsc, ludzi, czasu, lokalizacji itp. - każde kolejne słowo tagowane jest informacją mówiącą o tym, czy dane słowo jest częścią pożądanego przez nas typu (np. częścią lokalizacji), czy nie (np. z użyciem kodowania IOB, o którym mówiliśmy przy okazji CONLL)</li>\n",
        "    <li>Tagowanie częściami mowy - każde słowo otrzymuje etykietę mówiącą o tym jaka część mowy reprezentowana jest przez aktualny token.</li>\n",
        "    <li>Wykrywanie ważnych z naszego punktu widzenia fraz (nazwy produktów, technologii itp.)</li>\n",
        "    <li>...</li>\n",
        "</ol>\n",
        "\n",
        "Mówiąc o encjach nazwanych (Named Entities) - mówimy o frazach, którym nadaliśmy określony typ, np: \"01.06.2018\" - typ \"Data\", \"Poznań, Polska\" - typ \"Lokalizacja\", \"GeForce 1080 GTX Ultra\" - typ \"Sprzęt Komputerowy\".\n",
        "\n",
        "\n",
        "\n",
        "## Własny NER - trening z użyciem algorytmu CRF (Conditional Random Fields)\n",
        "\n",
        "Wykrywacze encji wytrenowane są do odnajdywania popularnych typów fraz (Daty, Lokalizacje, Osoby, ...). Co jednak, kiedy chcielibyśmy wykrywać zdefiniowane przez nas typy danych (np. sprzęt komputerowy), które nie są domyśnie wspierane przez istniejące modele? Musielibyśmy wytrenować własnego NERa. Użyjmy paczki 'pycrfsuite' do tego celu.\n",
        "\n",
        "PyCRFSuite implementuje algorytm CRF - bardzo wydajny algorytm, który potrafi uczyć się tagowania poszczególnych słów z użyciem np. kodowania IOB. Aby rozróżnić różne rodzaje encji, często tagi \"I\" i \"B\" kodowania IOB opatruje się dodatkowym sufiksem. Np. B-Date - oznacza początek daty, a I-Location - kontynuację frazy zawierającej lokację.\n",
        "\n",
        "Ponieważ to czy dane słowo jest encją nazwaną zależy zarówno od tego jak dane słowo wygląda, jak i od słów poprzedzających i następujących po aktualnym - w opisie cech CRFów również uwzględnia się informacje o okalających słowach.\n",
        "\n",
        "**Zadanie 3 (2 punkty)** Wytrenuj model, który będzie tagował poszczególne słowa w tekście z użyciem pycfrsuite. Aby to zrobić, wykonaj podzadania w krokach poniżej.\n",
        "\n",
        "Nasz NER będzie się uczyć etykiet na zbiorze tekstów hiszpańskich, które poddane są podziałowi na zdania, tokenizacji, tagowaniem częściami mowy i etykietami encji do wykrycia w formacie IOB. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8DIrzO2P6Rx",
        "outputId": "2a8a82ce-3cc9-488d-bbb0-de33fdbaaa54"
      },
      "source": [
        "!pip install python-crfsuite\n",
        "import nltk\n",
        "import sklearn\n",
        "import pycrfsuite\n",
        "\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train')) # załaduj korpus treningowy dla języka hiszpańskiego\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))  # załaduj korpus testowy dla języka hiszpańskiego\n",
        "train_sents[2] # wyświetla przykładowe zdanie, aby zobaczyć jak reprezentowane są dane"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.7/dist-packages (0.9.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('El', 'DA', 'O'),\n",
              " ('Abogado', 'NC', 'B-PER'),\n",
              " ('General', 'AQ', 'I-PER'),\n",
              " ('del', 'SP', 'I-PER'),\n",
              " ('Estado', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('Daryl', 'VMI', 'B-PER'),\n",
              " ('Williams', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('subrayó', 'VMI', 'O'),\n",
              " ('hoy', 'RG', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('necesidad', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('tomar', 'VMN', 'O'),\n",
              " ('medidas', 'NC', 'O'),\n",
              " ('para', 'SP', 'O'),\n",
              " ('proteger', 'VMN', 'O'),\n",
              " ('al', 'SP', 'O'),\n",
              " ('sistema', 'NC', 'O'),\n",
              " ('judicial', 'AQ', 'O'),\n",
              " ('australiano', 'AQ', 'O'),\n",
              " ('frente', 'RG', 'O'),\n",
              " ('a', 'SP', 'O'),\n",
              " ('una', 'DI', 'O'),\n",
              " ('página', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('internet', 'NC', 'O'),\n",
              " ('que', 'PR', 'O'),\n",
              " ('imposibilita', 'VMI', 'O'),\n",
              " ('el', 'DA', 'O'),\n",
              " ('cumplimiento', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('los', 'DA', 'O'),\n",
              " ('principios', 'NC', 'O'),\n",
              " ('básicos', 'AQ', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('Ley', 'NC', 'B-MISC'),\n",
              " ('.', 'Fp', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyTWGKbbP6Rx"
      },
      "source": [
        "**Zadanie 3a (1 punkt)** Tworzenie cech. PyCRFSuite oczekuje, że każde słowo opisane będzie zestawem odpowiednich cech w formie pythonowego słownika. Uzupełnij kod funkcji word2features (sekcje TODO) tak, aby stworzyć odpowiednie cechy zgodnie z nazwami i komentarzami do poszczególnych pól."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkAecUEgP6Ry"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]  # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'Ley'\n",
        "    postag = sent[i][1] # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'NC'\n",
        "    \n",
        "    features = {      # cechy aktualnego słowo\n",
        "        'bias': 1.0,\n",
        "        'lowercase_word': word.lower(), # TODO, tutaj słowo małymi literami\n",
        "        'word_last_3_chars': word[-3:], # TODO, tutaj ostatnie 3 znaki słowa\n",
        "        'word_last_2_chars': word[-2:], # TODO, tutaj ostatnie 2 znaki słowa\n",
        "        'word_is_uppercase': word.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "        'word_is_digit': word.isdigit(), # TODO, tutaj flaga (True/False), czy słowo jest liczbą\n",
        "        'postag': postag, # TODO, tutaj pos-tag (patrz początek definicji funkcji)\n",
        "        'postag_first_two_chars': postag[:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  \n",
        "    }\n",
        "    if i > 0:         # jeśli nasze słowo nie jest pierwszym w zdaniu - dodajmy do zbioru naszych cech cechy poprzedniego tokenu\n",
        "        word1 = sent[i-1][0]    # poprzednie słowo\n",
        "        postag1 = sent[i-1][1]  # poprzedni pos-tag\n",
        "        \n",
        "        features.update({       # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
        "            'previous_word_lower': word1.lower(), # TODO, tutaj poprzednie słowo małymi literami\n",
        "            'previous_word_is_upppercase': word1.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "            'previous_word_postag': postag1, # TODO, tutaj pos-tag poprzedniego słowa \n",
        "            'previous_word_postag_first_two_chars': postag1[:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  poprzedniego słowa\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True   # jeśli to pierwszy token - ustawmy cechę BOS (Begin of Sentence) na True\n",
        "        \n",
        "    if i < len(sent)-1:          # Jeśli nasze słowo nie jest ostatnim - dodajmy do zbioru cech cechy następnego słowa \n",
        "        word1 = sent[i+1][0]     # następne słowo\n",
        "        postag1 = sent[i+1][1]   # następny postag\n",
        "        \n",
        "        features.update({        # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
        "            'next_word_is_lower': word1.islower(), # TODO, tutaj flaga - czy następne słowo małymi literami\n",
        "            'next_word_is_upppercase': word1.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "            'next_word_postag': postag1, # TODO, tutaj pos-tag następnego słowa \n",
        "            'next_word_postag_first_two_chars': postag1[:2], # TODO, tutaj pierwsze 2 znaki pos-tagu  następnego słowa\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True   # jeśli to ostatni token - ustawmy cechę EOS (End of Sentence) na True\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))] # zamień każde słowo ze zdania na słownik cech"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myS4b1hJP6Ry",
        "outputId": "ec8a9024-c221-4e12-c8c7-b5e6525da1f4"
      },
      "source": [
        "sent2features(train_sents[0])[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BOS': True,\n",
              " 'bias': 1.0,\n",
              " 'lowercase_word': 'melbourne',\n",
              " 'next_word_is_lower': False,\n",
              " 'next_word_is_upppercase': False,\n",
              " 'next_word_postag': 'Fpa',\n",
              " 'next_word_postag_first_two_chars': 'Fp',\n",
              " 'postag': 'NP',\n",
              " 'postag_first_two_chars': 'NP',\n",
              " 'word_is_digit': False,\n",
              " 'word_is_uppercase': False,\n",
              " 'word_last_2_chars': 'ne',\n",
              " 'word_last_3_chars': 'rne'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1elvuKlP6Ry"
      },
      "source": [
        "Oczekiwany rezultat: \n",
        "<pre>\n",
        "{'BOS': True,\n",
        " 'bias': 1.0,\n",
        " 'lowercase_word': 'melbourne',\n",
        " 'next_word_lower': 'False',\n",
        " 'next_word_is_upppercase': False,\n",
        " 'next_word_postag': 'Fpa',\n",
        " 'next_word_postag_first_two_chars': 'Fp',\n",
        " 'postag': 'NP',\n",
        " 'postag_first_two_chars': 'NP',\n",
        " 'word_is_digit': False,\n",
        " 'word_is_uppercase': False,\n",
        " 'word_last_2_chars': 'ne',\n",
        " 'word_last_3_chars': 'rne'}\n",
        "</pre>\n",
        " \n",
        " **Zadanie 3b (1 punkt) - napisz ciała funkcji pomocniczych, które dla aktualnego zdania z train_sents i test_sents zwrócą:**\n",
        " <ul>\n",
        "     <li>sent2labels - zwróci ciąg oczkiwanych etykiet dla każdego wyrazu. parametr sent jest listą słów, z których każde słowo opisane jest trójką: tekst słowa, pos-tag słowa, etykieta słowa; np. ('Abogado', 'NC', 'B-PER') </li>\n",
        "     <li>sent2tokens - analogicznie do powyższego, jednak zamiast etykiet zwróci ciąg słów bez pos-tagów i etykiet.</li>\n",
        "     <li>get_all_labels - funkcja, która ze zbioru wszystkich zdań treningowych wyświetli zbiór etykiet (zbiór, czyli bez powtórzeń). Funkcja pokaże nam ilu etykiet chcemy się nauczyć, aby móc ocenić trudność naszego problemu.</li>\n",
        " </ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBeSZpbcP6Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfb9e0d-20ca-4456-e7ac-72dcbe04f6d2"
      },
      "source": [
        "def sent2labels(sent):\n",
        "  return [x[2] for x in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "  return [x[0] for x in sent]\n",
        "\n",
        "def get_all_labels(train_sents):\n",
        "  return {x[2] for xs in train_sents for x in xs}\n",
        "\n",
        "print(sent2labels(train_sents[0]))\n",
        "print(sent2tokens(train_sents[0]))\n",
        "print(get_all_labels(train_sents))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
            "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
            "{'I-LOC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'I-ORG', 'B-LOC', 'B-MISC', 'B-ORG'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSwOF2xzP6Rz"
      },
      "source": [
        "Oczekiwany rezultat:\n",
        "<pre>\n",
        "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
        "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
        "{'I-PER', 'I-MISC', 'B-LOC', 'I-LOC', 'B-PER', 'B-MISC', 'I-ORG', 'B-ORG', 'O'}\n",
        "</pre>\n",
        "\n",
        "Uruchom poniższy kod i sprawdź czego nauczył się nasz NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKV6B7PP6Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5a36d0-f175-4604-957f-fecf659e6656"
      },
      "source": [
        "X_train = [sent2features(s) for s in train_sents] # Stwórz cechy zbioru treningowego\n",
        "y_train = [sent2labels(s) for s in train_sents]   # Pobierz etykiety zbioru treningowego\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]   # Stwórz cechy zbioru testowego\n",
        "y_test = [sent2labels(s) for s in test_sents]     # Pobierz etykiety zbioru testowego\n",
        "\n",
        "trainer = pycrfsuite.Trainer(verbose=False)    # stwórz obiekt trenujący\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):       # iteruj po zdaniach i etykietach\n",
        "    trainer.append(xseq, yseq)                 # dopisuj do obiektu trenującego nasze dane\n",
        "    \n",
        "trainer.set_params({\n",
        "    'c1': 1.0,   # parametr regularyzacyjny L1\n",
        "    'c2': 1e-3,  # parametr regularyzacyjny L2\n",
        "    'max_iterations': 50,  # maksymalna liczba iteracji\n",
        "    # dodaj tranzycje, które nie są obserwowane ale są możliwe\n",
        "    'feature.possible_transitions': True\n",
        "})\n",
        "\n",
        "trainer.train('conll2002-esp.crfsuite')       # wytrenuj model i zapisz do pliku!\n",
        "\n",
        "tagger = pycrfsuite.Tagger()                  # stwórz tagger, który będzie nadawał etykiety naszej sekwencji\n",
        "tagger.open('conll2002-esp.crfsuite')         # załaduj do niego wytrenowany model\n",
        "example_sent = test_sents[0]                  # weź pierwsze z brzegu zdanie, które nie brało udziału w treningu\n",
        "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')   # wyświetl je...\n",
        "\n",
        "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))  # zobacz, co generuje nasz model\n",
        "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))                # i to, czego oczekiwano!"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La Coruña , 23 may ( EFECOM ) .\n",
            "\n",
            "Predicted: B-LOC I-LOC O O O O B-ORG O O\n",
            "Correct:   B-LOC I-LOC O O O O B-ORG O O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7xY5WXP6Rz"
      },
      "source": [
        "## Named entity recognition za pomocą sieci neuronowych:\n",
        "\n",
        "Jeśli zastanawiacie się jak zrobić NERa za pomocą sieci neuronowych (Keras), to na Kaggle jest świetny fragment kodu: https://www.kaggle.com/ananysharma/ner-with-bi-lstm\n",
        "\n",
        "Po ostatnich zajęciach ten kod powinien być prosty do zrozumienia :)\n",
        "\n",
        "Bi-LSTM to dwukierunkowe LSTMy (zerknijcie na ostatnie slajdy z wprowdzenia do RNN (laboratoria 6))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-SkFdspP6R0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5grk3knSP6R0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}