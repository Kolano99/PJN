{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Sieci Rekurencyjne.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC-rLp7U0c-k"
      },
      "source": [
        "# Sieci rekurencyjne (RNN)\n",
        "\n",
        "Klasyczne sieci neuronowe mają dwie zasadnicze wady, które sprawiają, że przetwarzanie języka naturalnego z ich udziałem jest problematyczne:\n",
        "<ul>\n",
        "    <li>Wyuczone wagi powiązane są z konkretnymi pozycjami cechy w wektorze cech</li>\n",
        "    <li>Ciężko uwzględnić relacje między cechami (n-gramy pomagają tylko trochę)</li>\n",
        "</ul>\n",
        "\n",
        "Kiedy rozważamy problemy przetwarzania języka - cechami są słowa w zdaniach, bądź jakieś statystyki powiązane ze słowami. Gramatyka większości języków (w szczególności fleksyjnych, takich jak nasz) pozwala jednak na pojawienie się istotnych wyrażeń w różnych miejscach w zdaniu. Kiedy chcielibyśmy wykonać zadanie wykrywania nazw firm w zdaniach - pozycja słowa w zdaniu dostarcza niewielu informacji o tym czy dane słowo jest rzeczywiście firmą czy nie.\n",
        "\n",
        "**Apple** jest najlepszą firmą.\n",
        "\n",
        "Najlepszą firmą jest **Apple**.\n",
        "\n",
        "Najlepsza firma - to **Apple**.\n",
        "\n",
        "Firma **Apple** jest najlepsza.\n",
        "\n",
        "Co więcej - często wcześniejsze słowa mają wpływ na kolejne. Wyobraźmy sobie zadanie uzupełniania luki w zdaniu. \n",
        "Mamy dwa zdania:\n",
        "\n",
        "\n",
        "W pracy zawodowej piszą bardzo dużo kodu, jestem [   ].\n",
        "\n",
        "W pracy zawodowej często pomagam ludziom, jestem [   ].\n",
        "\n",
        "Słowa poprzedzające luki pozwalają nam z dużo większym prawdopodobieństwem oszacować, że w pierwszym przypadku powinniśmy umieścić zawód programisty lub podobny, a w drugim - pielęgniarki lub podobny. Zatem poprzednie elementy sekwencji, mają wpływ na kolejne.\n",
        "\n",
        "Pomóc w takich problemach mogą sieci rekurencyjne.\n",
        "\n",
        "Dzisiejsze laboratoria pokażą jak zaimplementować sieć rekurencyjną od zera i jak nauczyć ją dodawania do siebie dwóch liczb reprezentowanych w postaci stringów (w formacie binarnym - jako ciągi zer i jedynek). (Np. 7=\"111\", 5=\"101\"). \n",
        "\n",
        "\n",
        "Dlaczego taki problem? Dodawanie dwóch reprezentacji binarnych jest bardzo prostym zadaniem, w którym sekwencja znaków jest istotna. Kiedy dodajemy dwie liczby binarne, często mamy do czynienia z potrzebą uwzględnienia bitu przeniesienia z aktualnego do kolejnego kroku (kiedy dodajemy do siebie 1 + 1) - nasza sieć neuronowa nauczy się uwzględniać informację o tym, ucząc się wykorzystania pamięci znajdującej się w warstwie ukrytej.\n",
        "\n",
        "![unfolded_rnn.png](attachment:unfolded_rnn.png)\n",
        "\n",
        "Powyższy obrazek pokazuje sieć rekurencyjną. Widzimy, że zawiera ona w sobie klasyczną sieć feedforward składającą się z 1 warstwy ukrytej (hidden layer) oraz następującej po niej warstwie wyjściowej (output layer). Aby przetworzyć sekwencję, ta sama sieć jest zwielokrotniona (w poziomie) i przetwarza po kolei każdy element z sekwencji. Na powyższym obrazku sieć feedforward (na czarno) ma 3 kopie - przetwarza więc sekwencję 3-elementową (np. 3 następujące po sobie słowa).\n",
        "\n",
        "Przekazywanie informacji o poprzednich elementach sekwencji (np. słowach) do kolejnych następuje poprzez połączenie warstwy ukrytej w kolejnych krokach czasowych (czerwone połączenia) Warstwa ukryta powinna modelować pamięć, która może być użyta do podejmowania przyszłych decyzji. \n",
        "\n",
        "Literki U, V, W symbolizują macierze wag pomiędzy warstwami. U łączy warstwę wejściową z ukrytą (każdy z każdym), V - łączy ukrytą z wyjściową (każdy z każdym), a W łączy warstwę ukrytą z poprzedniego punktu w czasie z kolejnym (również każdy z każdym).\n",
        "\n",
        "Warto zauważyć, że do przetworzenia każdego kroku sekwencji używamy dokładnie tych samych wag. Wagi U, V, W są użyte przy każdej \"kopii\" sieci rekurencyjnej (jak na obrazku). \n",
        "\n",
        "---\n",
        "\n",
        "![layers.jpg](attachment:layers.jpg)\n",
        "\n",
        "Sieci rekurencyjne stwarzają nam kilka możliwości tworzenia architektur. Powyżej zwizualizowanych jest kilka modeli z jedną warstwą ukrytą.\n",
        "\n",
        "Dane w sekwencji następują po sobie jeden po drugim (słowa tworzą uporządkowany ciąg), każdy z elementów ciągu nadchodzi później, reprezentuje więc późniejszy punkt w czasie. Oś pozioma (od lewej do prawej) na powyższym obrazku symbolizuje upływ czasu - kolejne kroki wynikające z przyjmowania/generowania kolejnych elementów sekwencji.\n",
        "\n",
        "Różowy prostokąt - wektor cech warstwy wejściowej, dla aktualnego punktu w czasie\n",
        "\n",
        "Zielony prostokąt - wektor warstwy ukrytej dla aktualnego punktu w czasie\n",
        "\n",
        "Niebieski prostokąt - wektor warstwy wyjściowej dla aktualnego punktu w czasie\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>one to one - klasyczna sieć feedforward, wektor cech transformowany jest do warstwy ukrytej, ta zaś transformowana jest do warstwy wyjśiowej - brak rekurencji, brak sekwencji</li>\n",
        "<li>one to many - tworzenie sekwencji z pojedynczego wektora cech. Np. tworzenie opisu obrazków mając zadany 1 obrazek (opis - sekwencja wielu wyrazów, wejście 1 obrazek)</li>\n",
        "<li>many to one - tworzenie pojedynczego wyjścia dla sekwencji wielu wejść. Np. Wykrywanie sentymentu w recenzji tekstowej. Z sekwencji wielu wejść (wiele słów recenzji) generujemy pojedynczą decyzję (sentyment powiązany z recenzją).</li>\n",
        "<li>many to many - Z sekwencji N wejść, tworzymy sekwencję M wyjść. Np. tłumaczenie maszynowe - na wejściu sekwencja słów w jednym języku, na wyjściu zaś przetłumaczona sekwencja słów (potencjalnie o innej długości!)</li>\n",
        "<li>many to many (synchronizowane) - generowanie sekwencji o długości takiej jak sekwencja wejściowa. Np. klasyfikacja kolejnych klatek wideo.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAxx22230c-r"
      },
      "source": [
        "## Materiały online:\n",
        "**Ponieważ, podobnie jak w przypadku poprzednich laboratoriów, najłatwiej byłoby pokazać działanie sieci rekurencyjnych przy tablicy, a trochę ciężej zrobić to za pomocą statycznych obrazków, polecam poniższy materiał na YouTube, który w intuicyjny sposób przedstawia ideę sieci rekurencyjnych i problemy z ich wykorzystaniem (https://www.youtube.com/watch?v=LHXXI4-IEns - długość ok. 10 minut)**\n",
        "\n",
        "## Zadanie1 (1.25 pkt): Inicjalizacja wag sieci\n",
        "\n",
        "Wagi na połączeniach między warstwami powinny być początkowo niewielkimi losowymi odchyleniami od wartości 0. W procesie uczenia wartości te modyfikować się będą tak, aby jak najdokładniej przewidywać oczekiwane wyjście.\n",
        "\n",
        "**Wykorzystując numpy, napisz funkcję, która wygeneruje losową macierz zadanych rozmiarów (rows - liczba wierszy, cols - liczba kolumn), której wartości będą zawarte w przedziale -1 do 1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xLOOQ1c0c-s",
        "outputId": "b5c822a0-4f51-414d-ca24-6e566edd5203"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "def generate_random_matrix(rows, cols):\n",
        "    return np.random.random((rows, cols))\n",
        "\n",
        "print(generate_random_matrix(4, 3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzljL-d10c-t"
      },
      "source": [
        "## Zadanie 2 (2.5 pkt): Wyznaczenie aktywacji warstwy ukrytej (pamięci) i wyjściowej\n",
        "\n",
        "W przypadku sieci rekurencyjnych, w przeciwieństwie do klasycznych feedforward - pojawia się połączenie rekursywne z historycznymi danymi. \n",
        "Jeśli stworzymy prostą rekurencyjną sieć neuronową z jedną warstwą ukrytą, to obliczenia w tej sieci wyglądać będą następująco:\n",
        "\n",
        "$\\vec{a_{h1(t)}} = sigmoid(U\\vec{x} + W\\vec{a_{h1(t-1)}} + \\vec{b_1})$ - aktywacja f. sigmoid jest tylko przykładem,  można użyć innej aktywacji (tanh, relu, etc)\n",
        "\n",
        "$\\vec{a_{o(t)}} = softmax(V\\vec{a_{h1(t)}} + \\vec{b_2})$ - softmax jest przykładem f. aktywacji, ma sens w problemie klasyfikacji przy > 2 etykietach, można również użyć np. sigmoid (jeśli jeden neuron na wyjściu)\n",
        "\n",
        "<br/>\n",
        "\n",
        "gdzie $\\vec{a_{h1(t)}}$ to wartość wektora reprezentującego aktywację warstwy ukrytej w aktualnym kroku,\n",
        "\n",
        "$\\vec{a_{h1(t-1)}}$ to wartość aktywacji warstwy ukrytej w poprzednim kroku,\n",
        "\n",
        "$\\vec{a_{o(t)}}$ to rezultat wygenerowany przez całą sieć neuronową (aktywacja na warstwie \"output\")\n",
        "\n",
        "<br/>\n",
        "Poprzez uwzględnienie wartości warstwy ukrytej z poprzedniego kroku - sieć może nauczyć się w jaki sposób poprzednie dane z sekwencji wpływają na aktualną decyzję.\n",
        "\n",
        "**Zadanie2: Mając zadane wartości macierzy wag: U, V, W a także wartości warstwy ukrytej z poprzedniego kroku oraz wektor cech - zaimplementuj dwie funkcje**:\n",
        "<ol>\n",
        "    <li>get_hidden_state - funkcja obliczająca aktualną wartość wektora w warstwie ukrytej (pamięć o sekwencji).</li>\n",
        "    <li>get_network_output - funkcja obliczająca wyjście sieci dla aktualnie obliczonych wartości w warstwie ukrytej</li>\n",
        "</ol>\n",
        "Dla prostoty późniejszych obliczeń - pomińmy wektor biasów, gdyż nie jest on niezbędny do rozwiązania naszego problemu, wzór na aktywację warstwy ukrytej bez biasów: $\\vec{a_{h1(t)}} = sigmoid(U\\vec{x} + W\\vec{a_{h1(t-1)}})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxRgijix0c-t",
        "outputId": "0e2d8524-f256-4205-aec9-cab21c15078b"
      },
      "source": [
        "# policz sigmoidę po wektorze lub skalarze\n",
        "def sigmoid(x): \n",
        "    output = 1 / (1 + np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# policz softmax po wektorze lub skalarze\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "U = np.array([[-3., 2., ], [4., 2., ], [1., -5.,]])\n",
        "W = np.array([[1.25, 1.3, 1.5], [2.01, 3.4, -2.4], [1.08, -.3, 0.1]])\n",
        "V = np.array([[-0.2,0.81, -0.2], [0.12, 0.42, 0.21], [0.1, 0.32, 0.01]])\n",
        "\n",
        "x = np.array([0.5, 0.21])\n",
        "prev_hidden = np.array([0.1, 0.32, 0.01])\n",
        "\n",
        "def get_hidden_state_activation(U, W, x, prev_hidden):\n",
        "    return sigmoid(np.dot(U, x) + np.dot(W, prev_hidden))\n",
        "\n",
        "def get_network_output(V, current_hidden):\n",
        "    return softmax(np.dot(V, current_hidden))\n",
        "\n",
        "\n",
        "hidden_state = get_hidden_state_activation(U, W, x, prev_hidden)\n",
        "print(hidden_state)\n",
        "print(get_network_output(V, hidden_state))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.37191738 0.97551727 0.36888573]\n",
            "[0.37811473 0.33866203 0.28322324]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrRVyiMn0c-u"
      },
      "source": [
        "Expected output: \n",
        "\n",
        "0.37191738 0.97551727 0.36888573\n",
        "\n",
        "0.37811473 0.33866203 0.28322324\n",
        "\n",
        "## Zadanie 3 (1.25 pkt)- ilość parametrów\n",
        "\n",
        "Mając zadaną sieć rekurencyjną o jednej warstwie ukrytej, z połączeniem rekurencyjnym w warstwie ukrytej **ile parametrów zostanie optymalizowanych podczas nauki?** Przyjmijmy, że sieć neuronowa ma 20 wejść, warstwa ukryta ma rozmiar 10 neuronów, a wyjściowa - 5 neuronów.\n",
        "\n",
        "a) Nie wliczając biasów (jak w zadaniu 2)\n",
        "\n",
        "b) Uwzględniając biasy na każdym neuronie\n",
        "\n",
        "Odpowiedzi zawrzyj w komentarzach poniżej"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "C4prGjcA0c-u"
      },
      "source": [
        "# Odp 3a: 10 * (20 + 10 + 5) = 350\n",
        "# Odp 3b: 3a + 5 + 10 = 350 + 15 = 365"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbfeT45H0c-u"
      },
      "source": [
        "---\n",
        "\n",
        "Poniżej znajduje się przykładowa implementacja kodu, który tworzy sieć neuronową, będącą w stanie dodawać do siebie dwie sekwencje binarnych reprezentacji liczb (dwie liczby w formacie binarnym). Tworzona sieć to sieć rekurencyjna, typu many-to-many (synchronizowana)*.\n",
        "\n",
        "(*) Jak wiemy, dodawanie dwóch liczb binarnych może dać na wyjściu liczbę, której reprezentacja binarna będzie dłuższa niż dwie liczby wejściowe, np. \"111 + 001 = 1000\" - nasz problem nie zawiera takich danych - wszystkie dane treningowe dobrane są tak, że długość wyniku nigdy nie będzie dłuższa niż długość wejść. Upraszcza to implementację. Ponadto zaimplementowana sieć nie uwzględnia biasów (nie są wymagane aby rozwiązać problem, a ich pominięcie sprawia, że kod jest bardziej \"kompaktowy\").\n",
        "\n",
        "**Uruchom poniższy kod, sprawdź jak uczy się sieć i przeanalizuj ten fragment kodu. Zadanie nie jest oceniane, ma tylko pokazać jak taka sieć może zostać zaimplementowana**\n",
        "\n",
        "!!! Uwaga: poniższy kod korzysta z Twoich implementacji funkcji **generate_random_matrix** oraz **get_hidden_state_activation**. Przed uruchomieniem tego kodu upewnij się, że poprawnie zaimplementował(e/a)ś obie funkcje!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZZgxzE80c-v",
        "outputId": "4e81bbec-39d6-4a82-8ee4-bca96aa19f6e"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# policz sigmoidę o wektorze lub skalarze\n",
        "def sigmoid(x): \n",
        "    output = 1 / (1 + np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# policz pochodną z sigmoidy\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output * (1 - output)\n",
        "\n",
        "# wczytaj dane wejściowe\n",
        "# w formacie liczba1_binarnie,liczba2_binarnie,ich_suma_binarnie\n",
        "# np. 0000111,0000001,0001000\n",
        "def load_dataset(path):\n",
        "    X = []  # w tej liście będziemy zapisywać pary składników\n",
        "    Y = []  # w tej liście będziemy zapisywać sumy składników\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            if len(line.strip()) == 0:\n",
        "                continue\n",
        "            input1_bin, input2_bin, sum_bin = line.strip().split(\",\")\n",
        "            \n",
        "            #wczytane dane to stringi, przekształć na wektory liczb\n",
        "            input1_bin = np.array([int(i) for i in input1_bin])\n",
        "            input2_bin = np.array([int(i) for i in input2_bin])\n",
        "            sum_bin = np.array([int(i) for i in sum_bin])\n",
        "            \n",
        "            X.append((input1_bin, input2_bin)) # zapisz pary składników jako wejścia\n",
        "            Y.append(sum_bin)                  # zapisz sumę jako oczekiwany rezultat\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "train_X, train_Y = load_dataset('dataset.csv')\n",
        "\n",
        "alpha = 0.1      # stała uczenia (learning_rate) - jak duże zmiany robić w uczeniu\n",
        "input_dim = 2    # ile cech (liczb) na wejściu sieci\n",
        "hidden_dim = 16  # rozmiar warstwy ukrytej (rozmiar wektora z pamięcią)\n",
        "output_dim = 1   # ile wartości na wyjściu \n",
        "\n",
        "\n",
        "# initialize neural network weights\n",
        "U = generate_random_matrix(input_dim, hidden_dim)   # inicjalizacja macierzy między wejściem a warstwą ukrytą\n",
        "V = generate_random_matrix(hidden_dim, output_dim)  # inicjalizacja macierzy między warstwą ukrytą a wyjściową\n",
        "W = generate_random_matrix(hidden_dim, hidden_dim)  # inicjalizacja macierzy między poprzednim stanem warstwy ukrytej a aktualnym\n",
        "\n",
        "U_update = np.zeros_like(U) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "V_update = np.zeros_like(V) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "W_update = np.zeros_like(W) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "\n",
        "for j in range(len(train_Y)):    # iterujemy po wszystkich przykładach uczących\n",
        "    added_one_seq = train_X[j][0]   # pierwszy składnik do sumowania w postaci binarnej, np. 00001\n",
        "    added_two_seq = train_X[j][1]   # drugi składnik do sumowania w postaci binarnej, np. 00010\n",
        "    expected_sum_seq = train_Y[j]   # oczekiwany wynik sumowania obu składników (dla przykładu wyżej: 00011)\n",
        "\n",
        "    predicted_sum_seq = np.zeros_like(expected_sum_seq) # tworzymy pusty wektor, który wypełniać będziemy wartościami 0 lub 1 tworząc naszą predykcję\n",
        "\n",
        "    overallError = 0                # tutaj będziemy zapisywać jak bardzo nasze przewidywania różnią się od oczekiwań  \n",
        "\n",
        "    output_l_deltas = list()\n",
        "    hidden_l_values = list() # wartości aktywacji warstwy ukrytej, zapisywane aby\n",
        "    hidden_l_values.append(np.zeros(hidden_dim))\n",
        "\n",
        "    # iterujemy po postaci binarnej bit po bicie, od najmniej znaczącego (od prawej do lewej)\n",
        "    for position in range(len(added_one_seq) - 1, -1, -1):\n",
        "\n",
        "        # jako wejście sieci w aktualnym kroku - bierzemy parę bitów na pozycji [position] (2 liczby)\n",
        "        X = np.array([\n",
        "            [added_one_seq[position], added_two_seq[position]]\n",
        "        ])\n",
        "        # jako oczekiwane wyjście sieci w aktualnym kroku - bierzemy bit na pozycji [position] (budujemy odpowiedź jako sekwencję, znak po znaku!)\n",
        "        y = np.array([[expected_sum_seq[position]]]).T\n",
        "\n",
        "        # Obliczamy wartość warstwy ukrytej\n",
        "        # uwaga, mały hack; w zależności od przygotowania parametrów U, V, W i danych wejściowych (ich orientacji wiersze - kolumny) x\n",
        "        # czasami zdarza się, że odwracamy kolejność operacji na macierzach, tak, aby rozmiary po przekształceniach\n",
        "        # 'pasowały do siebie', mnożenie Ux, może zamienić się na xU. W analizowanym przypadku jest podobnie.\n",
        "        # Dlatego też zamieniłem miejscami kolejność parametrów, podając X za U a hidden_values za W\n",
        "        # w ten sposób odwracamy kolejność mnożenia otrzymując XU + hidden_l_values[-1]V\n",
        "        # więcej informacji pod: https://medium.com/@vivek.yadav/wx-b-vs-xw-b-why-different-formulas-for-deep-neural-networks-in-theory-and-implementation-a5ae6995c4ef\n",
        "        hidden_l = get_hidden_state_activation(X, hidden_l_values[-1], U, W)\n",
        "        \n",
        "        # pobieramy wygenerowane wyjście sieci neuronowej (sigmoida, a nie softmax, gdyż mamy tylko jedno wyjście binarne)\n",
        "        predicted_char = sigmoid(np.dot(hidden_l, V))\n",
        "\n",
        "        # i sprawdzamy jak bardzo nasze przewidywanie się myli w stosunku do oczekiwanego znaku\n",
        "        output_l_error = predicted_char - y\n",
        "\n",
        "        output_l_deltas.append(\n",
        "            (output_l_error) * sigmoid_output_to_derivative(predicted_char)   # loss * grad out\n",
        "        )\n",
        "        overallError += np.abs(output_l_error[0]) # zwiększamy całościowy błąd o błąd z aktualnego kroku, bierzemy wartość bezwzględną, bo znak błędu (nadmiar/niedomiar) nie jest istotny.\n",
        "\n",
        "        predicted_sum_seq[position] = np.round(predicted_char[0][0]) # zapisujemy nasze przewidywanie na odpowiedniej pozycji, zaokrąglając (predicted_char to wartość rzeczywista między 0 a 1, zawiera prawdopodobieństwo tego, że liczba powinna być 1-nką) \n",
        "\n",
        "        hidden_l_values.append(copy.deepcopy(hidden_l)) # zapisz wartość aktywacji warstwy ukrytej, aby można było ją użyć w kolejnym kroku\n",
        "\n",
        "    future_hidden_l_delta = np.zeros(hidden_dim)        \n",
        "    \n",
        "    # propagacja wsteczna \n",
        "    for position in range(len(added_one_seq)):              \n",
        "        X = np.array([\n",
        "            [added_one_seq[position], added_two_seq[position]]\n",
        "        ])  # weź parę liczb od lewej do prawej\n",
        "        hidden_l = hidden_l_values[-position - 1] \n",
        "        prev_hidden_l = hidden_l_values[-position - 2]\n",
        "\n",
        "        # błąd na warstwie wyjściowej\n",
        "        output_l_delta = output_l_deltas[-position - 1]\n",
        "        # błąd na warstwie ukrytej\n",
        "        hidden_l_delta = (future_hidden_l_delta.dot(W.T) + output_l_delta.dot(V.T)) * sigmoid_output_to_derivative(hidden_l)\n",
        "\n",
        "        # zaktualizujmy macierze poprawek względem błędu w aktualnym kroku (na aktualnej pozycji w sekwencji)\n",
        "        V_update += np.atleast_2d(hidden_l).T.dot(output_l_delta)\n",
        "        W_update += np.atleast_2d(prev_hidden_l).T.dot(hidden_l_delta)\n",
        "        U_update += X.T.dot(hidden_l_delta)\n",
        "\n",
        "        future_hidden_l_delta = hidden_l_delta\n",
        "\n",
        "    U -= U_update * alpha  # spadek wag między wejściem a w. ukrytą\n",
        "    V -= V_update * alpha  # spadek wag między w. ukrytą a wyjściem\n",
        "    W -= W_update * alpha  # spadek wag między poprzednią a teraźniejszą w. ukrytą\n",
        "\n",
        "    U_update *= 0  # zeruj macierz jak profesjonalista\n",
        "    V_update *= 0  # zeruj macierz jak profesjonalista\n",
        "    W_update *= 0  # zeruj macierz jak profesjonalista\n",
        "\n",
        "    if(j % 5000 == 0):\n",
        "        print(\"Błąd przykładu:\" + str(overallError))\n",
        "        print(\"Przewidziana sekwencja:\" + str(predicted_sum_seq))\n",
        "        print(\"Oczekiwana sekwencja:  \" + str(expected_sum_seq))\n",
        "        print(\"------------\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Błąd przykładu:[5.00519366]\n",
            "Przewidziana sekwencja:[1 1 1 1 1 1 1 1]\n",
            "Oczekiwana sekwencja:  [0 1 0 0 0 1 0 1]\n",
            "------------\n",
            "Błąd przykładu:[3.96109629]\n",
            "Przewidziana sekwencja:[0 0 0 0 0 0 0 1]\n",
            "Oczekiwana sekwencja:  [1 1 0 0 0 0 1 0]\n",
            "------------\n",
            "Błąd przykładu:[3.16310633]\n",
            "Przewidziana sekwencja:[1 1 1 1 1 1 1 1]\n",
            "Oczekiwana sekwencja:  [0 1 0 1 1 1 1 1]\n",
            "------------\n",
            "Błąd przykładu:[3.53602575]\n",
            "Przewidziana sekwencja:[0 1 1 0 0 1 0 1]\n",
            "Oczekiwana sekwencja:  [0 1 1 0 1 0 0 1]\n",
            "------------\n",
            "Błąd przykładu:[3.26862727]\n",
            "Przewidziana sekwencja:[0 1 0 1 0 0 1 1]\n",
            "Oczekiwana sekwencja:  [1 0 0 1 1 0 1 1]\n",
            "------------\n",
            "Błąd przykładu:[1.74444822]\n",
            "Przewidziana sekwencja:[0 0 0 0 0 0 0 1]\n",
            "Oczekiwana sekwencja:  [0 1 0 0 0 1 0 1]\n",
            "------------\n",
            "Błąd przykładu:[1.45279543]\n",
            "Przewidziana sekwencja:[1 0 1 0 1 0 1 0]\n",
            "Oczekiwana sekwencja:  [1 0 1 0 1 0 1 0]\n",
            "------------\n",
            "Błąd przykładu:[0.53380328]\n",
            "Przewidziana sekwencja:[1 0 1 0 1 1 1 1]\n",
            "Oczekiwana sekwencja:  [1 0 1 0 1 1 1 1]\n",
            "------------\n",
            "Błąd przykładu:[0.22055326]\n",
            "Przewidziana sekwencja:[0 1 0 1 0 0 0 0]\n",
            "Oczekiwana sekwencja:  [0 1 0 1 0 0 0 0]\n",
            "------------\n",
            "Błąd przykładu:[0.45726048]\n",
            "Przewidziana sekwencja:[0 1 0 1 1 0 1 1]\n",
            "Oczekiwana sekwencja:  [0 1 0 1 1 0 1 1]\n",
            "------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "05yFbEZz0c-x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}